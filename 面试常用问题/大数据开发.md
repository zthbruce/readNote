# map-reduce过程
> map-reduce是Hadoop的核心计算模型，解决了大规模数据的处理问题，其核心思想是将大问题分解成小问题，且各个小问题之间可以相对独立

## 架构
> 从计算架构上来说，主要分为输入partition，map过程，shuffle过程， reduce过程
1. map
2. shuffle
3. reduce

## 流程
###. 输入分区(切片)：input split
> 从数据源(HDFS,Hbase等)获取数据，并将数据进行分区(切片)，每个分区(切片)由一个task负责(由AM的task_scheduler负责生成与分区数相同的task)
> 一般是一个block一个分区, 如此可方便mapper读取本地存储的block，减少网络传输I/O
> 具体的分割可以通过设置：mapred.min.split.size, mapred.max.split.size, block.size来控制

###. Map过程
> map本质上是本地的数据做了一个函数映射(转换)
> 一个mapper负责处理一个输入分区的数据

###. Shuffle过程
> shuffle过程解决了重新分区的问题，把特定key值的数据分配给一个特定的reducer(通过partitioner来控制)，形成一个分区有序的文件
> shuffle从架构上分为map-shuffle(shuffle write) 和 reduce-shuffle(shuffle read)
1. Partition
> 按照partion函数为每个map输出计算其partitioner，partitioner对应处理的reducer
> 默认情况下，采用Hash分区进行计算，可以自己编写partition函数

2. Collector
> Hadoop的map 输出的缓冲区kvBuffer(是一个环形的数据结构，能够更好的利用空间)
> 一旦占用超过阈值(io.sort.spill.percent，默认为0.8)，就会启动spill线程开始溢写数据，spill线程的名字是sortAndSpill，就是先排序后溢写

3. Sort
> 将KVBuffer中的的数据进行值按照key和partitionID进行二次排序，使得结果数据按照partitioner聚集，并且分区内按照key值升序

4. Spill
> hadoop为spill创建一个磁盘文件，将kvBuffer中的数据按照partition进行遍历溢写，生成spill.out和spill.out.index文件，index是索引文件，对每个分区保存三元信息(起始位置，原始长度，压缩长度)，如果这时设置了combiner，则会启动combine and spill，即先进行combine(将相同的key值进行合并，类似于本地reduce过程)，再进行溢写，该操作会减低溢写文件数

5. Merge
> map的数据量如果很大，则会导致溢写很多次，形成很多个spill.out和spill.out.index文件，Merge就是将溢写的文件进行合并
> 合并的过程也是按照partition进行遍历，将属于该partition的segment从磁盘上取出(遍历找到索引文件，然后取数据)；由于每个segment都是有序的，所以此处只需要进行归并即可，最终将所有的进行partition遍历完了，生成一个file.out和file.out.index。

MapShuffle至此结束，每个mapper会生成两个文件，file.out和file.out.index(索引文件，记录每个分区的起始和结束点)



6. Copy
> 每个reducer通过http请求，Reduce会定期向JobTracker(后来是AM)请求Map的输出位置，一旦获取完成，则reducer就会将数据复制到本地，不会等到所有Map任务结束

7. Merge Sort
> 复制过来之后，每个mapper上的数据是按照key值有序的，但是不同的mapper的数据合并在一块就不是有序的，要达到整体有序，只需要再次使用归并排序即可，一般都是一遍copy一遍sort, 如果该过程复制的数据过多，依然会产生溢写，如果设置了combiner，combine依然会生效

reduceShuffle就此结束

### Reduce阶段
1. Group
> 接下来就是聚集的过程，将同一个key值进行分组，有序的数据分组非常容易时间复杂度为O(N)
2. Reduce
> 将group后的数据<key, [v1, v2, v3, ...]> 喂给reduce函数，reduce函数进行相应的计算即可


## spark写一个word_count
val conf = new SparkConf()
val sc = new SparkContext(conf)
val lines = sc.textFile("hdfs://...")
val words = lines.flatMap(_.split(" ")) // 展开操作
val pairs = words.map((_, 1)) // 转换
val wordCounts = pairs.reduceByKey(_ + _) // 单词计数

wordCounts.collect().foreach(println(_))