# map-reduce过程
> map-reduce是Hadoop的核心计算模型，解决了大规模数据的处理问题，其核心思想是将大问题分解成小问题，且各个小问题之间可以相对独立

## 架构
> 从计算架构上来说，主要分为输入partition，map过程，shuffle过程， reduce过程
1. map
2. shuffle
3. reduce

## 流程
###. 输入分区(切片)：input split
> 从数据源(HDFS,Hbase等)获取数据，并将数据进行分区(切片)，每个分区(切片)由一个task负责(由AM的task_scheduler负责生成与分区数相同的task)
> 一般是一个block一个分区, 如此可方便mapper读取本地存储的block，减少网络传输I/O
> 具体的分割可以通过设置：mapred.min.split.size, mapred.max.split.size, block.size来控制

###. Map过程
> map本质上是本地的数据做了一个函数映射(转换)
> 一个mapper负责处理一个输入分区的数据

###. Shuffle过程
> shuffle过程解决了重新分区的问题，把特定key值的数据分配给一个特定的reducer(通过partitioner来控制)，形成一个分区有序的文件
> shuffle从架构上分为map-shuffle(shuffle write) 和 reduce-shuffle(shuffle read)
1. Partition
> 按照partion函数为每个map输出计算其partitioner，partitioner对应处理的reducer
> 默认情况下，采用Hash分区进行计算，可以自己编写partition函数

2. Collector
> Hadoop的map 输出的缓冲区kvBuffer(是一个环形的数据结构，能够更好的利用空间)
> 一旦占用超过阈值(io.sort.spill.percent，默认为0.8)，就会启动spill线程开始溢写数据，spill线程的名字是sortAndSpill，就是先排序后溢写

3. Sort
> 将KVBuffer中的的数据进行值按照key和partitionID进行二次排序，使得结果数据按照partitioner聚集，并且分区内按照key值升序

4. Spill
> hadoop为spill创建一个磁盘文件，将kvBuffer中的数据按照partition进行遍历溢写，生成spill.out和spill.out.index文件，index是索引文件，对每个分区保存三元信息(起始位置，原始长度，压缩长度)，如果这时设置了combiner，则会启动combine and spill，即先进行combine(将相同的key值进行合并，类似于本地reduce过程)，再进行溢写，该操作会减低溢写文件数

5. Merge
> map的数据量如果很大，则会导致溢写很多次，形成很多个spill.out和spill.out.index文件，Merge就是将溢写的文件进行合并
> 合并的过程也是按照partition进行遍历，将属于该partition的segment从磁盘上取出(遍历找到索引文件，然后取数据)；由于每个segment都是有序的，所以此处只需要进行归并即可，最终将所有的进行partition遍历完了，生成一个file.out和file.out.index。

MapShuffle至此结束，每个mapper会生成两个文件，file.out和file.out.index(索引文件，记录每个分区的起始和结束点)



6. Copy
> 每个reducer通过http请求，Reduce会定期向JobTracker(后来是AM)请求Map的输出位置，一旦获取完成，则reducer就会将数据复制到本地，不会等到所有Map任务结束

7. Merge Sort
> 复制过来之后，每个mapper上的数据是按照key值有序的，但是不同的mapper的数据合并在一块就不是有序的，要达到整体有序，只需要再次使用归并排序即可，一般都是一遍copy一遍sort, 如果该过程复制的数据过多，依然会产生溢写，如果设置了combiner，combine依然会生效

reduceShuffle就此结束

### Reduce阶段
1. Group
> 接下来就是聚集的过程，将同一个key值进行分组，有序的数据分组非常容易时间复杂度为O(N)
2. Reduce
> 将group后的数据<key, [v1, v2, v3, ...]> 喂给reduce函数，reduce函数进行相应的计算即可


## spark写一个word_count
val conf = new SparkConf()
val sc = new SparkContext(conf) // 初始化
val lines = sc.textFile("hdfs://...") // 取数据分片
val words = lines.flatMap(_.split(" ")) // 展开操作
val pairs = words.map((_, 1)) // 转换
val wordCounts = pairs.reduceByKey(_ + _) // 单词计数

wordCounts.collect().foreach(println(_))

## 每种Spark shuffle的使用场景，优势
> 不管是何种Shuffle方式，其实都是Shuffle Write and Shuffle read 两个阶段；
> Shuffle Read的机制变化不会特别大，但是Shuffle Write一直在变革，根据Shuffle write的方式进行分类可以分为三类：
Hash Shuffle, Sort-Shuffle, Tungsten-Sort Shuffle
> 本质上来说就两种，hash shuffle 和 sort shuffle， 但是sort又分为直接sort和Tungsten-Sort，我们先看看其发展

1. Spark 0.8以前
> Shuffle Write过程按照Hash的方式重组Partition的数据，不经过排序;
> 每个map task会为每个reduce task生成一个文件(对应M*R个文件)，伴随着大量的随机磁盘IO操作和大量的内存开销(句柄);
> Shuffle Read过程如果有combiner操作，那么它会把拉到的数据保存在一个Spark封装的哈希表（AppendOnlyMap）中进行合并

优化的Hash Shuffle
2. Spark 0.8.1为Hash Shuffle引入Consolidation机制
> 中间文件的数目修改为每个执行单位，每个执行单位(一般是core)会对每个reducer产生一个文件，同一个core会写到同一个文件中
> 配置参数 spark.shuffle.consolidateFiles

3. Spark 0.9 引入ExternalAppendOnlyMap
> shuffle read过程中不仅仅使用内存，如果产生溢写，可以spill到磁盘，最后进行堆排序merge

4. Spark1.1 引入 Sort Based Shuffle; 默认还是Sort-shuffle

5. Spark 1.2 默认的Shuffle方式改为Sort Based Shuffle

6. Spark 1.4 引入Tungsten-Sort Based Shuffle

7. Spark 1.6 Tungsten-sort并入Sort Based Shuffle
> 由SortShuffleManager自动判断选择最佳Shuffle方式，如果检测到满足Tungsten-sort条件会自动采用Tungsten-sort Based Shuffle，否则采用Sort Based Shuffle。

8. Spark 2.0 Hash Based Shuffle退出历史舞台

> Spark Shuffle都是分为Shuffle Read 和 Shuffle Write两个阶段，所以shuffle的分类是由Shuffle Writer进行分类的
> 下面看看三种shuffle方式的优缺点和适用场景
### Hash Based Shuffle
> Hash Shuffle是Spark1.2之前的默认Shufflse实现，并在Spark2.0之后彻底退出历史舞台；
> Hash Shuffle采用的是HashShuffleWriter；
> Shuffle的具体过程可以看看之前写的流程。下面主要关注过程中的一些细节
#### 持久化
> Spark为了实现容错，会将数据持久化, 数据持久化的参数选项: storage_level
1. MEMORY_ONLY
2. MEMORY_AND_DISK
3. MEMORY_ONLY_SER
4. MEMORY_AND_DISK_SER
5. DISK_ONLY
> RDD.unpersist()方法可以删除持久化

#### bucket数量(缓冲区)
> ShuffleMapTask会将结果写入bucket1, bucket2, bucket3，缓冲区最终一一写入文件，bucket的数目由什么决定？
> 由reducer的数目，即等于partitioner的numPartition数目，所以每个task会生成numPartition个文件
> 总量巨大！！！！
#### map端聚合(combine)
> 类似于map-reduce中的combiner，负责将map端的输出先进行聚合，将map端输出的相同的key值进行aggragate聚合.
> 因为不经过排序，所以使用的是AppendOnlyMap(功能类似于HashMap，不过底层只使用数组)，combine的逻辑为将task数据依次插入HashMap, 如果存在该key，就将键值更新为map[key]+value，如果不存在该key，就将键值更新为value

> 注意map端聚合有适用场景：只适合那些聚合的操作，类似于groupBy就不能使用
> 所以一般触发map端聚合的算子有：
reduceByKey,foldByKey,aggragateByKey, combineByKey

#### reduce端聚合
> map端聚合效率高？能有效减少磁盘和网络I/O(优化方向，文件记录能少则少);
> 但是map端聚合对一些聚合的算子并不能满足条件，所以在reduce端进行聚合也是不可避免，reduce端的聚合和map端的聚合原理相同;
> 参数mapSideCombine = false，表示只在reduce端进行聚合;

> map端聚合和reduce端聚合的区别在于：
reduce端聚合只会在reduce阶段进行局聚合计算，map端聚合将会在两端都进行聚合;
使用reduce端聚合的算子：GroupByKey(该算子不能在map端进行聚合)

> Spark调优(关于磁盘和网络IO)，尽可能使用map combine;

#### reduce端排序
> 如果使用了需要排序的transformation, 则会在reduce端进行排序;(否则默认是不排序)

> 实际上需要进行combine时，才是需要进行排序的(后面shuffleMap的时候)

> 怎么实现排序？
ExternalSorter内部维护了两个集合PartitionedPairBuffer和ParitionedAppendOnlyMap用以存储数据；
> 如果有aggragate则使用后者，否则使用前者存储数据；

> 这两类数据结构使用数组存储元素，2i位置存储的元组，内容为(PartitionID, key), 2i+1的位置存储的是value；
> 排序只需要对key进行排序即可，本质上来说就是对数组的排序，只不过需要将2i，和2i+1绑定起来，对偶数索引的key进行排序，然后将后面的元素进行移位；
> 排序算法：org.apache.spark.util.collection.TimSort

> 触发reduce端排序的算子为SortByKey, sortBy, repartitionAndSortWithinPartitions

#### spill
map端聚合，reduce端聚合，sort都可能产生溢写，因为可能内存里装不下，就先将数据进行溢写，等需要时再进行合并？
不应该先进行聚合再进行溢写么，没错就是这样

#### Consolidation
> 优化的Hash Shuffle，并非一个task产生R个文件，而是Executor中的核数(并发运行线程数)来决定产生文件的个数
#### 优缺点：使用场景
> 优点
1. map端写入时不需要进行排序
> 缺点
1. 消耗大量的内存(存储句柄)，磁盘I/O，和网络
2. Spark任务规模上不去(即使使用了FileConsolidation，最终也会因为reducer的需求量多，而导致开销过大)
> 使用场景:
适合小规模数据的使用

> 于是Sort Based Shuffle应运而生
### Sort Based Shuffle
> 此处主要对Sort Shuffle的write进行详述：源码见下图：
> Hash-Shuffle目前已经退出历史舞台了，Sort-Based-Shuffle为默认Shuffle方式，Sort-Based-Shuffle有三种模式，UnsafeShuffleWriter，BypassMergeSortShuffleWriter，SortShuffleWriter，运行时会根据情况进行动态选择

![write](sort-shuffle-write.png)
####. dep.mapSideCombine: 是否需要map阶段进行本地combiner
    i. 如果需要，那么需要传入aggregator和keyOrdering // reduceByKey走该路
        aggregator用于聚合计算，keyOrdering用于对key值进行排序的规则
    ii. 如果不需要，那么本地每个分区的数据不会做sort // group by走该路
        对于不需要进行combine的算子，没有必要将key值进行排序，排序反而会增大开销(如map-reduce)

####. ExternalSorter::insertAll()
> ExternalSorter在reduce端需要进行排序时，必然会用到，是个非常有用的类，内部使用tim-sort进行排序;
> External内部有三个非常重要的成员：
    i. PartitionedAppendOnlyMap(映射关系<key, value>, 如果需要聚合则采用该数据结构)
    ii. PartitionedPairBuffer(<key, ArrayBuffer>, 如果不需要聚合则采用该数据结构)
    ii. insertAll: 这个方法非常重要，下面解析这个方法

> Tungsten-Sort的数据结构里没有Map这种结构，所以不能使用聚合，但是Sort-Shuffle可以
> 首先，献上源码一览：
> 
    def insertAll(records: Iterator[Product2[K, V]]): Unit = {
    // TODO: stop combining if we find that the reduction factor isn't high
    val shouldCombine = aggregator.isDefined

    if (shouldCombine) {
        // Combine values in-memory first using our AppendOnlyMap
        val mergeValue = aggregator.get.mergeValue
        val createCombiner = aggregator.get.createCombiner
        var kv: Product2[K, V] = null
        val update = (hadValue: Boolean, oldValue: C) => {
        if (hadValue) mergeValue(oldValue, kv._2) else createCombiner(kv._2)
        }
        while (records.hasNext) {
        addElementsRead()
        kv = records.next()
        map.changeValue((getPartition(kv._1), kv._1), update)
        maybeSpillCollection(usingMap = true)
        }
    } else {
        // Stick values into our buffer
        while (records.hasNext) {
        addElementsRead()
        val kv = records.next()
        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[C])
        maybeSpillCollection(usingMap = false)
        }
    }
}

> 根据上面的条件，决定了insertAll方法中的参数，insertAll会生成spill文件, 所以需要知道是否要求排序
1. 如果定义了aggregator，那么需要进行数据的本地combiner: createCombiner或者mergeValue
    i. 从RDD的iterator(迭代器数据流)中不断读取数据<key, value>
    ii. 在data数组中查找是否已经存在该key，若存在则调用mergeValue, 不存在则调用createCombiner,hash会根据探测法检测
    iii. maybeSpillCollection(using map): 使用appendOnlyMap作为数据结构
         maybeSpill(map, map.estimateSize):
            > 判断内存是否够用：尝试向shuffleMemorManager申请内存，如果申请到了，则返回，申请不到，则spill到磁盘
            > spill(collection)  
                spillToMergeableFile(collection): 将数据集写到可以合并的文件里面
            > 如果是appendOnlyMap做spill，会先根据partitionID和key值进行排序
            > WritablePartitionedIterator.fromIterator(): 返回可写入文件的迭代器
    iv. it.writeNext(writer) // 开始写文件

2. 如果没有定义aggregator,则不需要combiner // group by会走该步
    i. 如果不需要combiner，且reducer数目小于200,则会默认走bypassMergeSort，该方法和Hash Shuffle的原理一模一样，但是最终将Shuffle的文件合并称为一个文件，可以为sort,groupby等操作进行提速，而且也算的上是一个single sorted file(按分区排序)
    ii. 如果不需要combiner，且reducer数目较大，或者需要进行排序，使用PartitionedPairBuffer作为内部数据结构，不需要支持支持的combine动作

#### 写分区文件 sorter.writePartitionedFile(blockId,context,outputFile):
1. 如果是bypassMergeSort,且有分区文件，则将内存中的数据调用spillToPartitionFiles()写入spill文件，然后简单合并所有文件(因为一个分区写一个文件)

2. 如果只有内存数据，没有spill file，只需要将内存数据排序(如果需要排序，则利用orderBy对key值进行排序，否则只需要按照分区进行排序)，然后写入文件

3. merge
> merge这一步非常重要，其实原理上和map-reduce的merge相同，遍历每个分区，如果需要combine对象必然spill出来的分区有序的文件
, 否则只需要之间合并即可

> reduce task读取时，其实不关心顺序，为何此处还要进行排序？

> 主要原因是merge的时候，mergeCombiner需要排序之后的数据(所以本质上还是需要combine的原因);

> 将内存中指定分区的数据读取出来, 再通过SpillReader将当前该读取的分区数据读取出来，然后合并, 形成新的iterator,这个iterator只有单分区的数据;
    i. 获取所有spill_file // val readers=spills.map(new SpillReader(_))
    ii. 获取内存中的所有数据 // val inMemBuffered=inMemory.buffered
    iii. 定义并返回一个(分区号, Iterator(K,C))的iterator, 这个iterator的next()操作(实际上就是遍历分区): 
        a) 如果需要aggregator，那么此时需要进行mergeCombiner操作
            mergeWithAggregation(iterators,aggregator.get.mergeCombiners,keyComparator,ordering.isDefined))> 如果没有定义totalOrder(不需要全局有序，分区有序):
                则会生成一个Iterator对象，该对象的iterator的next：
                > sorted是一个从小根对读取数据的iterator(存在多个数据), 每次从 sorted中读取数据; 用keyComparator对多路数据进行归并,取出当前key最小的数据,以及和这个key的hashcode相等的所有key, 然后精确比较找出和每个key相等的所有数据, 找到一条数据就会进行mergeCombiners合并;
            > 如果定义了totalOrder(全局有序，只需要按照当前顺序读取对应的key值然后进行combine即可)
                val sorted=mergeSort(iterators,comparator).buffered;  //还是用keyComparator生成读取小根对的iterator对象;
                > 后面的操作, 认为数据是已经按照totalorder 做好了排序, 所以hashcode相等的key必定相等, 这样就是顺序读取key相同的数据, 做mergeCombiner操作;(如果key值相等就进行合并，当进行一遍combine之后，则将)
            > 实际上就是存在溢写时，才需要进行全局合并，合并应该是通过一个
            > 总结来说，是一个多路归并的过程

        b) 如果不需要aggregator，但是需要排序：ordering.isDefined(生成小根堆进行排序)
            > val sorted=mergeSort(iterators,comparator).buffered; (进行归并排序)
            > 则只需要生成小根堆的迭代对象即可，后面都是按照迭代对象的方式进行取数，所以小根堆即可
            (还记的最小堆的生成方式)

        c) 如果不需要aggregator，也不需要排序
            > 则按照key的hashcode顺序输出即可(只要hashcode相同,就在同一个分区里面)


> 总的来说，生成的方式和Map-Reduce的shuffle方式比较相似，但Spark还是做了很大优化，比如说，将需要聚合的进行排序， 将不需要聚合的算子执行时，减少不必要的排序。

> 一般的任务都是有聚合操作的，所以能使用map端combine的算子就是用map端combine的算子

> 为什么使用Sort-Shuffle
1. 增大Spark框架的计算规模(否则计算规模会被限制，task数目多，reducer的数目也少不了)
2. 增大Spark集群的规模(主要是Executor的数目和core的数目可以上去了，否则采用consolidate方式，依然会达到瓶颈)

### Tungsten-Sort Based Shuffle
> 该Shuffle方式是属于sort的大类，Spark在运行时会动态的选择使用Sort-Based还是Tungsten-Sort Based Shuffle
> 该方式可以优化Spark内存和CPU；
> 因为使用java UnSafe API，所以在shuffle方式又被称为UnsafeShuffleManager

#### 存储
> 该shuffle的存储结构不同于partitionedAppendOnlyMap结构，引入了两种新的数据结构；(类似于一种OS)
1. ShuffleExternalSorter 
使用MemoryBlock存储数据(以page的方式进行存储,类似于操作系统），每条记录包括长度信息和K-V Pair;
2. ShuffleInMemorySorter 
> 使用long数组存储每条数据对应的分区，页码和条数，共8byte(相当于对指针进行排序)

#### 排序
> 写文件或者溢写前根据数据的PartitionId信息，使用TimSort对ShuffleInMemorySorter的long数组进行排序，排序结果为将相同的PartitionId聚集在一起，PartitionId较小的排在前面，ShuffleExternalSorter中的数据不需要处理

#### 写文件
> 依次读取ShuffleInMemorySorter中的long数组元素，再根据page number和offset信息去ShuffleExternalSorter中读取K-V Pair写入文件, 生成一个dataFile(按分区进行排列的文件)

#### 溢写 & 合并
1. 内存不足，会将数据溢写到磁盘，每次溢写会生成上述的一个dataFile
2. 每次map shuffle task数据处理结束后，会将merge合并成一个dataFile,作为数据处理结果


#### 优点
1. ShuffleExternalSorter使用Unsafe API操作序列化数据，而非java对象，减少了内存占用及因此导致的GC耗时，该优化需要Serializer支持relocation
2. ShuffleExternalSorter存原始数据，ShuffleInMemorySorter使用压缩指针存储元数据，每条记录仅占8 bytes，并且排序时不需要处理原始数据，效率高。
3. Spill和Merge这一步不需要反序列化数据

#### 使用场景
> 必须满足的条件，Spark通过这个方式进行动态选择：
1. map-side aggregation 否
2. Partition数(RDD) 小于16777216
3. Serializer支持relocation 是

> 这些条件在阐述优点的时候就已经表现出来原因了：
1. map-side aggregation: 存储的数据结构有关系，不是map型，而是数组型，无法在map-side进行合并
2. Partition数(RDD)：这由ShuffleInMemorySorter中8byte的partitionId的长度决定，24bit正好对应这个数
3. Serializer支持relocation：Serializer支持relocation是指，Serializer可以对已经序列化的对象进行排序，这种排序起到的效果和先对数据排序再序列化一致，该属性会在排序时用到


### Shuffle比较和选择
1. 没有map端聚合操作，且RDD的Partition数小于200，使用BypassMergeSortShuffleWriter(HashShuffle原理一模一样，就是最后将数据进行合并了)。

2. 没有map端聚合操作，RDD的Partition数小于16777216，且Serializer支持relocation，使用UnsafeShuffleWriter。

3. 上述条件都不满足，使用SortShuffleWriter。

## 说一说ExternalSorter中的timSort
> 在Spark Shuffle内部进行排序时，会采用timSort进行排序，Java SE后来使用的排序也是该排序，python2.3就开始使用timSort做为list的sort方式了;
> timSort实际上mergeSort的改进，虽然时间复杂度依然是O(N*logN)，但是在效率上和速度上却有很大提高，主要得益于以下几个方向的改进：
1. 合并过程能不能更快
2. 合并次数能不能减少(充分利用数组本身有序的性质，而不是递归到最小数组时才开始合并)
3. 能不能换成比mergeSort更简单的排序
> timSort就是解决了以上三个问题，因为这三个问题都是可以进行优化的，先祭上sort的代码: 
> 
    public void sort(Buffer a, int lo, int hi, Comparator<? super K> c) {
        // java的assert用法，异常情况的判断
        assert c != null;
        // 未排序的数组长度
        // 主要此处的hi表示数组的最大索引为hi-1,即不是最大的索引，但用于计算长度却是恰好
        int nRemaining  = hi - lo; 
        // 若数组大小为 0 或者 1
        // 那么就以及排序了
        if (nRemaining < 2)
        return;  

        // 若是小数组
        // 则不使用归并排序
        if (nRemaining < MIN_MERGE) {
        // 得到递增序列的长度
        int initRunLen = countRunAndMakeAscending(a, lo, hi, c);
        // 二分插入排序
        binarySort(a, lo, hi, lo + initRunLen, c);
        return;
        }
        // 栈
        SortState sortState = new SortState(a, c, hi - lo);
        // 得到最小run长度
        int minRun = minRunLength(nRemaining);
        do {
        // 得到递增序列的长度
        int runLen = countRunAndMakeAscending(a, lo, hi, c);

        // 若run太小，
        // 使用二分插入排序
        if (runLen < minRun) {
            int force = nRemaining <= minRun ? nRemaining : minRun;
            binarySort(a, lo, lo + force, lo + runLen, c);
            runLen = force;
        }

        // 入栈
        sortState.pushRun(lo, runLen);
        // 可能进行归并
        sortState.mergeCollapse();

        // 查找下一run的预操作
        lo += runLen;
        nRemaining -= runLen;
        } while (nRemaining != 0);

        // 归并所有剩余的run，完成排序
        assert lo == hi;
        sortState.mergeForceCollapse();
        assert sortState.stackSize == 1;
    }
> 该排序从架构上主要分为：划分run，合并run两个阶段，这两个阶段和mergeSort的方式一模一样，但是两个阶段都都进行了优化；
> 程序上不采用递归(函数栈)，而使用显式栈，存储划分的run；
### 优化点1: 数组size较小时，不适用归并排序，而是用二分插入排序(binarySort)，在size较小时，归并不如使用二分插入排序
> 根据待排序数组的长度进行判断，主要包括两个关键函数：1. countRunAndMakeAscending(获取最大递增子长度) 2. binarySort(二分插入排序)
#### countRunAndMakeAscending
    // 参数为数组，开始，结束，比较器
    private int countRunAndMakeAscending(Buffer a, int lo, int hi, Comparator<? super K> c) {
        assert lo < hi;
        int runHi = lo + 1;
        if (runHi == hi)
            return 1;

        K key0 = s.newKey();
        K key1 = s.newKey();

        // 找到run的尾部
        if (c.compare(s.getKey(a, runHi++, key0), s.getKey(a, lo, key1)) < 0) { 
        // 若是递减的，找到尾部反转run
            while (runHi < hi && c.compare(s.getKey(a, runHi, key0), s.getKey(a, runHi - 1, key1)) < 0)
                runHi++;
            reverseRange(a, lo, runHi); // 数组反转算法(前后索引，进行交换即可)
        }else{                              
            while (runHi < hi && c.compare(s.getKey(a, runHi, key0), s.getKey(a, runHi - 1, key1)) >= 0)
                runHi++;
        }
        // 返回run的长度
        return runHi - lo;
    }

> 该方法的目的是为了找到当前数组的最大递增子数组
> 如果是递减的，那么找到递减的最后索引，然后进行反转
> 如果是递增的，那么找到递增的最后索引
> 将数组返回接口
#### binarySort
> 
  private void binarySort(Buffer a, int lo, int hi, int start, Comparator<? super K> c) {
    assert lo <= start && start <= hi;
    if (start == lo)
      start++;

    K key0 = s.newKey();
    K key1 = s.newKey();

    Buffer pivotStore = s.allocate(1);
    // 将位置[start,hi)上的元素二分插入排序到已经有序的[lo,start)序列中
    for ( ; start < hi; start++) {
      s.copyElement(a, start, pivotStore, 0);
      K pivot = s.getKey(pivotStore, 0, key0);

      int left = lo;
      int right = start;
      assert left <= right;
      while (left < right) {
        int mid = (left + right) >>> 1;
        if (c.compare(pivot, s.getKey(a, mid, key1)) < 0)
          right = mid;
        else
          left = mid + 1;
      }
      assert left == right;

      int n = start - left;  
      // 对插入做简单的优化
      // 数据右移进行插入即可
      switch (n) {
        case 2:  s.copyElement(a, left + 1, a, left + 2);
        case 1:  s.copyElement(a, left, a, left + 1);
          break;
        default: s.copyRange(a, left, a, left + 1, n);
      }
      s.copyElement(pivotStore, 0, a, left);
    }
  }

> 只做一件事：将位置[start,hi)上的元素二分插入排序到已经有序的[lo,start)序列中，使其有序即可。

> 充分利用了其本身有序的特点，即可用起二分查找优化原来的插入排序：
1. 找到要插入的位置(O(logN))，
2. 然后将该位置以后的进行右移(O(N))，
3. 将该位置的数据更新即可完成;

### 划分run阶段
> 划分run阶段实际上是将一个数组分为n个run阶段，遍历数组生成run，然后添加到栈中等待下一步操作。
> run是有最小长度的，先找出最大递增子数组，然后与minRunLen进行比较即可，至少要达到minRunLen;

#### minRunLength：取得最小长度
  private int minRunLength(int n) {
    assert n >= 0;
    int r = 0;     
    // 这里 MIN_MERGE 为 2 的某次方
    // if n < MIN_MERGE ,
    // then 直接返回 n
    // else if n >= MIN_MERGE 且 n（>1） 为 2 的某次方，
    // then n 的二进制低位第1位 为 0，r |= (n & 1) 一直为 0 ，即返回的是  MIN_MERGE / 2
    // else  r 为之后一次循环的n的二进制低位第1位值 k ，返回的值 MIN_MERGE/2< k < MIN_MERGE 
    // 位运算
    while (n >= MIN_MERGE) {
      r |= (n & 1);
      n >>= 1;
    }
    return n + r;
  }
> 此处其实为了解决一个问题：即如何让run合并之后不能太小，设置一个阈值Min_MERGE，使最终得到的长度k>MIN_MERGE;
> 这样一来，就能使得合并之后>MIN_MERGE;
> 利用不断二分的方式进行求解，最终得到的run的长度 MIN_MERGE/2< k < MIN_MERGE;

#### 若run太小，则使用二分插入排序，将长度扩充为minRun
> 
      if (runLen < minRun) {
        int force = nRemaining <= minRun ? nRemaining : minRun;
        binarySort(a, lo, lo + force, lo + runLen, c);
        runLen = force;
      }

>  将其扩充到minRun的长度

#### 将run做入栈处理
>     private void pushRun(int runBase, int runLen) {
      this.runBase[stackSize] = runBase;
      this.runLen[stackSize] = runLen;
      stackSize++;
    }

> 设置两个栈，一个栈保存run的开始索引，一个栈保存run的长度；

#### 归并run
1. 使用sortState.mergeCollapse()进行run归并的方式
>  
    private void mergeCollapse() {
      while (stackSize > 1) {
        int n = stackSize - 2;
        if ( (n >= 1 && runLen[n-1] <= runLen[n] + runLen[n+1])
          || (n >= 2 && runLen[n-2] <= runLen[n] + runLen[n-1])) {
          if (runLen[n - 1] < runLen[n + 1])
            n--;
        } else if (runLen[n] > runLen[n + 1]) {
          break; 
        }
        mergeAt(n);
      }
    }

2. 使用mergeForceCollapse() 归并所有剩余的run
> 将第3个run
private void mergeForceCollapse() {
    // 将所有的run合并
      while (stackSize > 1) {
        int n = stackSize - 2;
        // 若第3个run 长度 小于 栈顶的run
        // 先归并第2,3个 run
        if (n > 0 && runLen[n - 1] < runLen[n + 1])
          n--;
        mergeAt(n);
      }
    }

3. mergeAt
> 这是merge的核心代码，优化了归并排序的流程，看下面这一段代码：
      // 从 run1 中找到 run2的第1个元素的位置
      // 在这之前的run1的元素都可以被忽略
      int k = gallopRight(s.getKey(a, base2, key0), a, base1, len1, 0, c);
      assert k >= 0;
      base1 += k;
      len1 -= k;
      if (len1 == 0)
        return;

      // 从 run2 中找到 run1的最后1个元素的位置
      // 在这之后的run2的元素都可以被忽略
      len2 = gallopLeft(s.getKey(a, base1 + len1 - 1, key0), a, base2, len2, len2 - 1, c);
      assert len2 >= 0;
      if (len2 == 0)
        return;
      // 归并run
      // 使用 min(len1, len2) 长度的临时数组
      if (len1 <= len2)
        mergeLo(base1, len1, base2, len2);
      else
        mergeHi(base1, len1, base2, len2);

> 这是归并方式的优化: 
> 相当于在run1中找到run2的第一个元素的位置(可以采用二分查找的方式)，那么run1中该位置之前的元素都不需要判断了；
> 找到run2中run1的最后一个元素的位置，那么在这之后的run2之后的元素就不再需要进行判断；
> 对剩余的部分进行归并排序即可：内部进行的优化？

4. 归并的逻辑
> Timsort会合并在栈中2个连续的run。X、Y、Z代表栈最上方的3个run的长度（图2），当不能同时满足下面2个条件是，X、Y这两个run会被合并，直到同时满足下面2个条件，则合并结束：
(1) X>Y+Z
(2) Y>Z

> 例如：如果X<Y+Z，那么X+Y合并为一个新的run，然后入栈。重复上述步骤，直到同时满足上述2个条件。当合并结束后，Timsort会继续找下一run，然后找到以后入栈，重复上述步骤，及每次run入栈都会检查是否需要合并2个run。


> timSort会有两次归并，一次是在run新入栈时，第二次是在最后整体的归并。


### 解决的问题
1. 减少归并的次数：归并的对象是栈里的run，run是连续递增的段，而且优化了run的长度，长度有限制，可以增加归并的效率(减少次数)
2. 优化归并的逻辑： 
简单的合并算法是用简单插入算法，依次从左到右或从右到左比较，然后合并2个run, 这个优化方向为：先找到一头一尾(利用。
    > 相当于在run1中找到run2的第一个元素的位置(可以采用二分查找的方式)，那么run1中该位置之前的元素都不需要判断了；
    > 找到run2中run1的最后一个元素的位置，那么在这之后的run2之后的元素就不再需要进行判断；(将run1的头，run2的尾掐掉，中间的进行归并排序)
3. 在数据规模较小时，采用二分插入排序的方式进行排序(优化规模)
> 二分插入排序(会优化传统的插入排序)

## Spark的排序：sortByKey内部是怎么实现的？
> sortByKey的核心思想为: 每个计算节点(mapper)对输入的数据进行重新分片(repartition),分片采用的是range partitio的方式进行分区，采用range-partitioning使得重新分片后的数据自然在分片之间是排好序的，但是每个分片内的数据不会进行排序(因为可能位于不同的计算节点上)，重新分片的数据通过Shuffle交换到新的计算节点上(next_stage，reducer)，每个reduce把自己负责的分片内的数据进行排序，然后就达到了全局排序的结果。

> sortByKey的重头戏在于range-partition，rangePartition的操作代价较大(所以在选择算子的时候能不使用就不适用这种方法)

1. Stage 0：Sample。创建 RangePartitioner，先对输入的数据的key做sampling来估算key的分布情况，然后按指定的排序切分出range，尽可能让每个partition对应的range里的key的数量均匀。计算出来的 rangeBounds 是一个长为 numPartitions - 1 的Array[TKey]，记录头 numPartitions - 1 个partition对应的range的上界；最后一个partition的边界就隐含在“剩余”当中。


2. Stage 1：Shuffle Write。开始shuffle，在map side做shuffle write，根据前面计算出的rangeBounds来重新partition。Shuffle write出的数据中，每个partition内的数据虽然尚未排序，但partition之间已经可以保证数据是按照partition index排序的了。

> 在sortByKey的排序情况下使用了BypassMergeSortShuffleWriter，把焦点聚焦到key如何分配到Partitioner和每个Partition的文件将会如何写入key，value生成Shuffle文件，在这两点上BypassMergeSortShuffleWriter将明显的不同于SortShuffleWrite。
> 该种shuffle方式下在map端是不会进行排序的

> 当然，如果分区数过多的情况下，BypassMergeSortShuffleWriter将达到瓶颈，这时会将采用SortShuffleWrite。

SortShuffleWriter并不一定会对数据按照key值排序，但一定会按照partitionID进行排序，事实上，只有当需要进行合并的情况下，才会进行排序, 便于combine

3. Stage 2：Shuffle Read。然后到reduce side，每个reducer再对拿到的本partition内的数据做排序。(ExternaSorter，划重点)

4. Stage 3: merge。 driver收到所有的result task，然后按照不同的partition进行合并，最后整体有序。

### reduce端的实现
> 即Stage2中Shuffle Read获取了数据之后怎么排序？
> BlockStoreShuffleReader中的read方法负责读取shuffle write的数据，在sortByKey中ExternalSorter会对收到的本partition内的数据做排序；

> 又提到ExternalSorter这个组件了，之前Sort-based Shuffle中就用过，是spill时为了溢写是否需要排序来实现的，该组件的最终要的架构为两种数据结构和一个方法：
1. PartitionedAppendOnlyMap(需要聚合操作combine)
2. PartitionedPairBuffer(不需要聚合操作)
3. insertAll
> insertAll 负责将数据写到内存里面(内存中数据结构的选择取决于是否需要聚合操作)

> 其实无论在map端还是reduce端，都会使用ExternalSorter，并用对应的数据结构来存储缓冲数据，如果内存装不下了，就采用溢写的方式存储在磁盘上，当做临时文件。

> 对于sortByKey来说，使用的数据结构PartitionedPairBuffer，对于reduceByKey这类的算子，是需要利用PartitionedAppendOnlyMap这种数据结构来进行聚合的。在Spark上实现的Map和Array都是用了数组的结构进行存储

> PartitionedPairBuffer在结构上类似于hadoop的K-V buffer, 使用数组来存储键值对，其中Key为二元组(partitionId, key), value即为对应的值，所以在排序时，就涉及到排序哪个值了，一般来说，Sort-Base-Shuffle是指按照partitionId进行排序，只有当需要聚合操作的时候才对key值进行排序。

> Shuffle Read一般的操作都是使用AppendOnlyMap，因为需要聚合运算，即使是groupByKey也是使用value = buffer++value的方式进行聚集，只有sort的时候的时候不需要聚集。

#### insertAll
> 将从ShuffleMapTask中获取的数据会先利用insertAll方法写入缓冲区，数据结构采用PartitionedPairBuffer进行存储。
1. 插入数据时，不会进行排序
2. 当内存不够时，采用溢写的方式保存到磁盘上，溢写之前，会将数据进行排序，至少按照partitionID进行排序，如果指定了按照key进行排序，那么需要进行二次排序。

#### 排序
> 当insertAll完成之后，构建一个排序的迭代器。
> 按照是否存在溢写文件分成两种情况：
1. 还在内存里没有Spill到文件中去，这时候构建一个内存里的PartitionedDestructiveSortedIterator迭代器，在迭代器中已经排序好了PartitionPairBuffer里的内容

2. Spill到文件里的，文件里的已经排好序了，需要对内存里的PartitionPairBuffer进行排序（和前面一种情况相同的处理），最后对文件和内存进行外排序(外排序本质上是一个归并排序，只是经过了一些列设计)

#### merge
> 最后driver通过获取的result task(排序后的结果)

> 在runJob按照partitionID进行合并，形成整体有序的集合打印到本地

> 构建了一个数组result，将每个Partition的数值保存到result的数组里
result[0]=partition[0] =array(tuple<k,v>,tuple<k,v>.....)

> 最终的合并需要依靠collect算子来进行合并


## HDFS的块为什么是64M
> 主要有三个原因：从寻址，并发度，元数据内存占用三个角度来实现

1. 拥有更小的块会导致寻址(查找)的开销增大
> 寻址时间10ms，传输速率为100MB/S，为了使寻址时间为传输时间的1%，则需要使块的大小为100M左右，现在默认为128M。
> 随着现在磁盘传输速度的提升，块的大小还会增加

2. 拥有太大的块，会导致并发任务较少，并发度降低，使得任务趋于单线程

3. 它减小了主存储器中存储的元数据的大小

## Spark的join使用场景，实现原理
> Join在数据库里简直是最基础的操作，利用join进行特征字段之间的连接

> Spark中有以下几种常用的join
1. join(innerJoin)
2. leftOuterJoin
3. rightOuterJoin
4. fullOuterJoin(全连接)

> 说到Join等或导致Shuffle Dependency算子，那必须从ReduceBuKey和GroupByKey这两个算子说起，这两个算子的区别类似于Array和Map的区别，因为涉及到是否做聚合操作，所以采用的数据结构不同

### reduceByKey
> reduceByKey的过程类似于Map-Reduce: partition，map, write，sort，spill，merge，copy，mergeSort,reduce；
> 区别在于，Map-Reduce的combine需要人为设置，Map-Reduce的学习成本比较高(能玩的转的必须深入了解Hadoop的map-reduce原理)；
> Spark对算子的封装和优化，使得Spark的学习成本会比较低。

#### 注意点
1. 不是所有数据fetch过来才开始进行聚合计算，数据流流到哪，哪就开始计算，即边fetch边聚合计算；

2. 还有一点值得注意的是，shuffledRDD(就是完成了Shuffled的数据)，通过mapPartition/mapPartitionWithContext的方式，对整个分区进行处理，进行聚合的func计算；
> mapPartition和map的区别在于粒度的不同，mapPartition的func是针对整个分区的，map是针对分区内单个元素的(比如进行聚合运算，就需要一个分区的进行计算，如果只是转换的话，可以使用map方法)

3. 如果需要map size combine，则会在map端也进行mapPartition的方式，进行聚合计算(一个分区内的计算)

## hdfs数据读写
### 读取流程，实现原理
1. client向NameNode发起请求
2. 通过RPC调用NameNode获取请求的文件块和文件块的位置(RPC请求，再返回数据)
3. client直接和DataNode进行交互获取数据
4. client关闭连接

### 写入流程
> 注意写入的时候的这个临时文件，写入要比读入更加复杂一点

1. 客户端在向NameNode请求之前先写入文件数据到本地文件系统的一个临时文件
2. 待临时文件达到数据块大小时开始向NameNode请求DataNode信息
3. NameNode在文件系统中创建文件并返回给客户端一个数据块及其对应的DateNode的地址列表
4. 客户端得到DateNode的地址将临时文件块flush到列表的第一个DataNode
5. NameNode提交这次文件创建，此时，文件在文件系统中可见


