# map-reduce过程
> map-reduce是Hadoop的核心计算模型，解决了大规模数据的处理问题，其核心思想是将大问题分解成小问题，且各个小问题之间可以相对独立

## 架构
> 从计算架构上来说，主要分为输入partition，map过程，shuffle过程， reduce过程
1. map
2. shuffle
3. reduce

## 流程
###. 输入分区(切片)：input split
> 从数据源(HDFS,Hbase等)获取数据，并将数据进行分区(切片)，每个分区(切片)由一个task负责(由AM的task_scheduler负责生成与分区数相同的task)
> 一般是一个block一个分区, 如此可方便mapper读取本地存储的block，减少网络传输I/O
> 具体的分割可以通过设置：mapred.min.split.size, mapred.max.split.size, block.size来控制

###. Map过程
> map本质上是本地的数据做了一个函数映射(转换)
> 一个mapper负责处理一个输入分区的数据

###. Shuffle过程
> shuffle过程解决了重新分区的问题，把特定key值的数据分配给一个特定的reducer(通过partitioner来控制)，形成一个分区有序的文件
> shuffle从架构上分为map-shuffle(shuffle write) 和 reduce-shuffle(shuffle read)
1. Partition
> 按照partion函数为每个map输出计算其partitioner，partitioner对应处理的reducer
> 默认情况下，采用Hash分区进行计算，可以自己编写partition函数

2. Collector
> Hadoop的map 输出的缓冲区kvBuffer(是一个环形的数据结构，能够更好的利用空间)
> 一旦占用超过阈值(io.sort.spill.percent，默认为0.8)，就会启动spill线程开始溢写数据，spill线程的名字是sortAndSpill，就是先排序后溢写

3. Sort
> 将KVBuffer中的的数据进行值按照key和partitionID进行二次排序，使得结果数据按照partitioner聚集，并且分区内按照key值升序

4. Spill
> hadoop为spill创建一个磁盘文件，将kvBuffer中的数据按照partition进行遍历溢写，生成spill.out和spill.out.index文件，index是索引文件，对每个分区保存三元信息(起始位置，原始长度，压缩长度)，如果这时设置了combiner，则会启动combine and spill，即先进行combine(将相同的key值进行合并，类似于本地reduce过程)，再进行溢写，该操作会减低溢写文件数

5. Merge
> map的数据量如果很大，则会导致溢写很多次，形成很多个spill.out和spill.out.index文件，Merge就是将溢写的文件进行合并
> 合并的过程也是按照partition进行遍历，将属于该partition的segment从磁盘上取出(遍历找到索引文件，然后取数据)；由于每个segment都是有序的，所以此处只需要进行归并即可，最终将所有的进行partition遍历完了，生成一个file.out和file.out.index。

MapShuffle至此结束，每个mapper会生成两个文件，file.out和file.out.index(索引文件，记录每个分区的起始和结束点)



6. Copy
> 每个reducer通过http请求，Reduce会定期向JobTracker(后来是AM)请求Map的输出位置，一旦获取完成，则reducer就会将数据复制到本地，不会等到所有Map任务结束

7. Merge Sort
> 复制过来之后，每个mapper上的数据是按照key值有序的，但是不同的mapper的数据合并在一块就不是有序的，要达到整体有序，只需要再次使用归并排序即可，一般都是一遍copy一遍sort, 如果该过程复制的数据过多，依然会产生溢写，如果设置了combiner，combine依然会生效

reduceShuffle就此结束

### Reduce阶段
1. Group
> 接下来就是聚集的过程，将同一个key值进行分组，有序的数据分组非常容易时间复杂度为O(N)
2. Reduce
> 将group后的数据<key, [v1, v2, v3, ...]> 喂给reduce函数，reduce函数进行相应的计算即可


## spark写一个word_count
val conf = new SparkConf()
val sc = new SparkContext(conf) // 初始化
val lines = sc.textFile("hdfs://...") // 取数据分片
val words = lines.flatMap(_.split(" ")) // 展开操作
val pairs = words.map((_, 1)) // 转换
val wordCounts = pairs.reduceByKey(_ + _) // 单词计数

wordCounts.collect().foreach(println(_))

## 每种Spark shuffle的使用场景，优势
> 本质上来说就两种，hash shuffle 和 sort shuffle， 但是sort又分为直接sort和Tungsten-Sort，我们先看看其发展

1. Spark 0.8以前
> Shuffle Write过程按照Hash的方式重组Partition的数据，不经过排序;
> 每个map task会为每个reduce task生成一个文件(对应M*R个文件)，伴随着大量的随机磁盘IO操作和大量的内存开销(句柄);
> Shuffle Read过程如果有combiner操作，那么它会把拉到的数据保存在一个Spark封装的哈希表（AppendOnlyMap）中进行合并

优化的Hash Shuffle
2. Spark 0.8.1为Hash Shuffle引入Consolidation机制
> 中间文件的数目修改为每个执行单位，每个执行单位(一般是core)会对每个reducer产生一个文件，同一个core会写到同一个文件中
> 配置参数 spark.shuffle.consolidateFiles

3. Spark 0.9 引入ExternalAppendOnlyMap
> shuffle read过程中不仅仅使用内存，如果产生溢写，可以spill到磁盘，最后进行堆排序merge

4. Spark1.1 引入 Sort Based Shuffle; 默认还是Sort-shuffle

5. Spark 1.2 默认的Shuffle方式改为Sort Based Shuffle

6. Spark 1.4 引入Tungsten-Sort Based Shuffle

7. Spark 1.6 Tungsten-sort并入Sort Based Shuffle
> 由SortShuffleManager自动判断选择最佳Shuffle方式，如果检测到满足Tungsten-sort条件会自动采用Tungsten-sort Based Shuffle，否则采用Sort Based Shuffle。

8. Spark 2.0 Hash Based Shuffle退出历史舞台

> Spark Shuffle都是分为Shuffle Read 和 Shuffle Write两个阶段，所以shuffle的分类是由Shuffle Writer进行分类的
> 下面看看三种shuffle方式的优缺点和适用场景
### Hash Based Shuffle
> Hash Shuffle是Spark1.2之前的默认Shufflse实现，并在Spark2.0之后彻底退出历史舞台；
> Hash Shuffle采用的是HashShuffleWriter；
> Shuffle的具体过程可以看看之前写的流程。下面主要关注过程中的一些细节
#### 持久化
> Spark为了实现容错，会将数据持久化, 数据持久化的参数选项: storage_level
1. MEMORY_ONLY
2. MEMORY_AND_DISK
3. MEMORY_ONLY_SER
4. MEMORY_AND_DISK_SER
5. DISK_ONLY
> RDD.unpersist()方法可以删除持久化

#### bucket数量(缓冲区)
> ShuffleMapTask会将结果写入bucket1, bucket2, bucket3，缓冲区最终一一写入文件，bucket的数目由什么决定？
> 由reducer的数目，即等于partitioner的numPartition数目，所以每个task会生成numPartition个文件
> 总量巨大！！！！
#### map端聚合(combine)
> 类似于map-reduce中的combiner，负责将map端的输出先进行聚合，将map端输出的相同的key值进行aggragate聚合.
> 因为不经过排序，所以使用的是AppendOnlyMap(功能类似于HashMap，不过底层只使用数组)，combine的逻辑为将task数据依次插入HashMap, 如果存在该key，就将键值更新为map[key]+value，如果不存在该key，就将键值更新为value

> 注意map端聚合有适用场景：只适合那些聚合的操作，类似于groupBy就不能使用
> 所以一般触发map端聚合的算子有：
reduceByKey,foldByKey,aggragateByKey, combineByKey

#### reduce端聚合
> map端聚合效率高？能有效减少磁盘和网络I/O(优化方向，文件记录能少则少);
> 但是map端聚合对一些聚合的算子并不能满足条件，所以在reduce端进行聚合也是不可避免，reduce端的聚合和map端的聚合原理相同;
> 参数mapSideCombine = false，表示只在reduce端进行聚合;

> map端聚合和reduce端聚合的区别在于：
reduce端聚合只会在reduce阶段进行局聚合计算，map端聚合将会在两端都进行聚合;
使用reduce端聚合的算子：GroupByKey(该算子不能在map端进行聚合)

> Spark调优(关于磁盘和网络IO)，尽可能使用map combine;

#### reduce端排序
> 如果使用了需要排序的transformation, 则会在reduce端进行排序;

> 怎么实现排序？
ExternalSorter内部维护了两个集合PartitionedPairBuffer和ParitionedAppendOnlyMap用以存储数据；
> 如果有aggragate则使用后者，否则使用前者存储数据；

> 这两类数据结构使用数组存储元素，2i位置存储的元组，内容为(PartitionID, key), 2i+1的位置存储的是value；
> 排序只需要对key进行排序即可，本质上来说就是对数组的排序，只不过需要将2i，和2i+1绑定起来，对偶数索引的key进行排序，然后将后面的元素进行移位；
> 排序算法：org.apache.spark.util.collection.TimSort

> 触发reduce端排序的算子为SortByKey, sortBy, repartitionAndSortWithinPartitions

#### spill
map端聚合，reduce端聚合，sort都可能产生溢写，因为可能内存里装不下，就先将数据进行溢写，等需要时再进行合并？
不应该先进行聚合再进行溢写么，没错就是这样

#### Consolidation
> 优化的Hash Shuffle，并非一个task产生R个文件，而是Executor中的核数(并发运行线程数)来决定产生文件的个数
#### 优缺点：使用场景
> 优点
1. map端写入时不需要进行排序
> 缺点
1. 消耗大量的内存(存储句柄)，磁盘I/O，和网络
2. Spark任务规模上不去(即使使用了FileConsolidation，最终也会因为reducer的需求量多，而导致开销过大)
> 使用场景:
适合小规模数据的使用

> 于是Sort Based Shuffle应运而生
### Sort Based Shuffle
> Sort Based Shuffle之后，并不是说结果是进行
> 在阅读了Spark源码之后，发现Sort Based Shuffle的原理上是将每个task的数据按照分区进行排序，对于需要map端combine的情况
> 需要进行排序，方便进行combine，因为combine是针对key值来进行aggragate

### Tungsten-Sort Based Shuffle