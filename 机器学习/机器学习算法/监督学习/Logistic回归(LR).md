## 算法背景(为什么)
> 线性回归是最基础的预测方法，但是其值却是连续的，不适合分类
> 如何改进线性回归，将之改进成一个线性分类算法，最原始的是二分类问题

## 使用了什么优化方法？解决了什么问题？
> 使用了logistic distribution的特点

> 利用sigmod函数将线性回归的结果映射到0-1的数，来表现其概率大小
> 所以说逻辑斯迪克回归之前，必须先说说线性回归模型

### 线性回归
> 线性回归分析用来描述自变量X和因变量Y之间的关系，或者说X对Y的影响程度，并对Y进行预测
> 通常使用最小二乘法实现对参数的估计，最小二乘法是典型的使用损失函数的方法，最小话损失函数来对参数进行估计

### 逻辑斯迪克回归
> 逻辑回归在线性回归的基础上了增加了一个逻辑sigmod函数，来表现因变量的概率(使用 1 / (1 + e^(- w * x )))
> 为什么这个概率密度函数是有效的，可以用全概率的公式进行解释
> 从设计理念上来说，只要一个的概率大于另外一个的概率即可，即可作为判别的条件
> 所以一般对于二分类来说 if P(y=1|x)>0.5 则将其判别为1
> 对于多分类来说，需要比较各个概率值大小即可

#### 个人理解逻辑斯迪克的回归
> 自变量X，应变量Y，他们之间的线性回归函数
> 如果要进行二分类，可以P(Y=1|X) = p, P(Y=0|X) = 1 - p
> P(Y=1|X) / P(Y=0|X) = p / (1-p) 其取值范围是为0~无穷大
> 我们希望用X的线性回归来表示 P(Y=1|X) / P(Y=0|X)的值，但是X的线性回归的值的取值范围是负无穷大~正无穷大，需要一个函数映射到0~无穷大, 很显然可以采用指数函数exp，而且该函数是递增的(我们希望越大，就越容易判别)
> 也就是说让 P(Y=1|X) / P(Y=0|X) = p / (1-p) = exp(w * x)
> 即可推出判别函数 P(Y=1|X) = p = exp(w * x) / (1 + exp(w * x)) = 1 / (1 + exp(- w * x))
> 通常的判别条件是：P(Y=1|X)是否>0.5
> 以上就是如何从线性回归转到逻辑斯迪克回归进行判别